\subsection{The SCL Monitor}
\inlineCSP

There are two main limitations to the standard JVM monitor: it suffers from spurious wakeups and does not allow targeting of |notify| calls. Spurious wakeups are obviously bad and are a common source of bugs where not adequately protected against. Targeting of signals can also be very beneficial to the performance of a program; take for example the one-place buffer shown below in Listing \ref{code::slot}. Suppose we have significantly more threads wanting to |get| a value than |put| a value. Each |notifyAll| will awake every thread waiting on a |get|, even if |get|s are blocked by |! available|. This results in the majority of the threads immediately sleeping again, adding significant overhead. A |notifyAll| is also required since the JVM monitor makes no guarantees as to which thread is awoke by a |notify|; as a result repeated |notify|s could potentially just wake up two thread alternatively, both of which are waiting to perform the same process.

% \begin{lstlisting}[language=Scala, label=code::slot, caption={Single placed buffer as an example of the inefficiency of untargeted signals}]
\begin{scala}[label=code::slot, caption={Single placed buffer as an example of the inefficiency of untargeted signals}]
  class OneBuff[T] {
    private var buff = null.asInstanceOf[T]
    private var available = false

    def put(x: T) = synchronized {
      while(available) wait()
      buff = x
      available = true
      notifyAll()
    }

    def get : T = synchronized {
      while(! available) wait()
      available = false
      notifyAll()
      return x // not overwritable as we still hold the lock
    }
  }
\end{scala}

The SCL monitor implementation solves both of these issues with a single monitor offering multiple distinct |Conditions| to allow for more targeted signalling. It is worth noting that these improvements result in the SCL monitor being more computationally expensive per call\framebox{Improve sentence}. Each individual condition offers the following operations:
\begin{itemize}
  \item |await()| is used to wait for a signal on the condition
  \item |signal()| is used to signal to a thread waiting on the condition
  \item |signalAll()| is used to signal to all the threasd waiting on the condition
\end{itemize}
Each of these operations should be performed while holding the lock. We can note that these operations are similar to the JVM monitor |wait()|, |notify()| and |notifyAll()| respectively. This functionality is also similar to the |java.util.concurrent.locks.Condition| class; the primary difference is that the SCL monitor blocks spurious wakeups whereas they are allowing by the JAVA class.

Considering the single-placed buffer, we can use two conditions to separate the threads attempting to |get| and those trying to |put|. We can then modify the program above to only perform a single |signal| towards the threads that are attempting to perform the opposing function, resulting in significant efficiency gains as no threads need to immediately sleep after being woken up.

The implementation of the SCL Monitor \cite{GitHub}, makes use of the Java |LockSupport| class; we explore this in the next section\framebox{Sub?}. Here we present a low-level model of an SCL Condition which makes use of a model of the |LockSupport| class.

\subsubsection{The SCL monitor implementation}

The SCL monitor is implemented in two sections: these are a central |Lock| and conditions associated with the central |Lock|. The |Lock| definition is a re-entrant lock (i.e.~ one thread can acquire the acquire the lock multiple times whilst it holds it) with a variable |locker: Thread| which indicates which thread currently holds the |Lock|. The lock implementation is quite simple and for modelling we will instead use a non-reentrant |Lock|; this will reduce the size of the FDR model significantly for model checking without removing functionality.

We instead focus on the |Condition| class. This internally uses a queue of |ThreadInfo| objects to store the identities of the waiting threads and a Boolean indicating if that thread has been singalled. When a thread calls |await()| it releases the |lock| and enqueues a |ThreadInfo| object into the queue of the respective condition. A signalling thread dequeues a |ThreadInfo| object from the queue (if non-empty), sets the |ThreadInfo|'s |ready| value to |true| indicatign that it has been signalled and then unparks the corresponding thread. |signalAll| acts similaly to |signal|, but repeats the above process for all |ThreadInfo| objects in the queue. We note that we have excluded all code regarding interruptions from the SCL code below: this is to reduce the complexity of the resulting CSP model. The interruption handling is also quite simple - it stops the thread from parking again once it has been interrupted and throws a |InterruptedException|.

\begin{scala}[caption={A subset of the {\scalastyle Condition} class from \cite{GitHub}}, label={scala:Condition}]
import java.util.concurrent.locks.LockSupport

/** A condition associated with `lock`. */
class Condition(lock: Lock){
  /** Information about waiting threads. */
  private class ThreadInfo{
    val thread = Thread.currentThread //the waiting thread
    @volatile var ready = false // has this thread received a signal?
  }

  /** Check that the current thread holds the lock. */
  @inline private def checkThread = 
    assert(Thread.currentThread == lock.locker, 
      s"Action on Condition by thread ${Thread.currentThread}, but the "+
        s"corresponding lock is held by ${lock.locker}")

  /** Queue holding ThreadInfos for waiting threads.
    * This is accessed only by a thread holding lock. */
  private val waiters = new scala.collection.mutable.Queue[ThreadInfo]()

  /** Wait on this condition. */
  def await(): Unit = {
    checkThread
    // record that I'm waiting
    val myInfo = new ThreadInfo; waiters.enqueue(myInfo) 
    val numLocked = lock.releaseAll                 // release the lock
    while(!myInfo.ready){
      LockSupport.park()                            // wait to be woken
    }                              // reacquire the lock
    lock.acquireMultiple(numLocked)      
  }

  /** Signal to the first waiting thread. */
  def signal(): Unit = {
    checkThread
    if(waiters.nonEmpty){
      val threadInfo = waiters.dequeue()
      if(!threadInfo.ready){
        threadInfo.ready = true; LockSupport.unpark(threadInfo.thread)
      }
      else signal() // try next one; that thread was interrupted or timed out
    }      
  }

  /** Signal to all waiting threads. */
  def signalAll() = {
    checkThread
    while(waiters.nonEmpty){
      val threadInfo = waiters.dequeue()
      threadInfo.ready = true; LockSupport.unpark(threadInfo.thread)
    }      
  }
}
\end{scala}

\subsubsection{The SCL monitor model}

\framebox{DIAGRAM}

We now consider our model of the SCL monitor. This consists of three main components: 
\begin{itemize}
  \item The monitor lock: as described above, we model the re-entrant lock of the implementation with a single entry lock here. This is because a thread could obtain the lock an unbounded number of times, resulting in a potentially unbounded state space in FDR. 
  
  The lock we use is a simple process \CSPM{Lock(m) = acquire.m?t -> release.m.t -> Lock(m)} which specifies that only one thread can hold the lock and that same thread must release the lock before some other thread can obtain it. This provides mutual exclusion between threads as required.
  \item The |LockSupport| module, described in section \ref{section:LockSupport}.
  \item The queue of |ThreadInfo| values, desribed in section \ref{section:ThreadInfo}.
\end{itemize}

\paragraph{LockSupport}\label{section:LockSupport}

The Java |LockSupport| \framebox{REF} class offers two main operations: |park| and |unpark|, which are used to suspend and resume a thread respectively. A thread |t| typically calls |LockSupport.park()| to suspend itself until a permit becomes available by another thread calling |LockSupport.unpark(t)|. As this is a permit-based system, a permit is stored if a thread calls |unpark(t)| and |t| is not yet parked. This allows |t| to immediately resume when it does call |park|.
It is also important to note that |LockSupport| is also affected by spurious wakeups where some parked thread can resume without an |unpark|. 
%We will therefore use the channel |spurious| to indicate that a spurious wakeup has occured; this will allow us to check that.

Threads interact with the |LockSupport| module via three main events: a thread can park itself, a thread can unpark another thread and a parked thread can wake up. We therefore introduce channels |park|, |unpark| and |wakeUp| to represent these three synchronisations. We also use the |spurious| channel to indicate that the following wake-up is spurious; this will allow us to check that |LockSupport| can only diverge when an infinite numebr of spurious wake-ups occur.

Internally, the model stores two sets of threads: |waiting| stores the parked threads and |permits| stores the threads with permits available. A thread that is parking is either added to the waiting set if no permit is available or it is immediately re-awoken is a permit is available. A thread that is performing an |unpark(t)| will either awake |t| if it is currently waiting or store a permit for |t|. We split our definition into two processes: |LockSupport| can either nondeterministically allow a waiting thread to spuriously wake-up, or it can act as the deterministic |LockSupport1| which initially only communicates |park| or |unpark| events.
%When there is at least one thread waiting and no ongoing |wakeup|, the |LockSupport| module can nondeterministically choose to either operate as normal or to allow one of the waiting threads to spuriously wakeup.
\framebox{GAVIN???}

\begin{cspm}[caption={The CSP model of the Java LockSupport module}]
module LockSupport

  channel park: ThreadID
  channel unpark: ThreadID.ThreadID
  channel wakeUp: ThreadID.Bool

  LockSupport1(waiting, permits) =
    park?t-> (
      if member(t, permits)
        then wakeUp.t -> LockSupport(waiting, diff(permits, {t}))
      else LockSupport(union(waiting, {t}), permits) )
    []
    unpark?t?t2-> ( 
      if member(t2, waiting)
        then wakeUp.t2 -> LockSupport(diff(waiting, {t2}), permits)
      else LockSupport(waiting, union(permits, {t2})))
  LockSupport(waiting, permits) =
    if waiting == {} then LockSupport1(waiting, permits)
    else (LockSupport1(waiting, permits)
          |~| spurious -> wakeUp$t:waiting -> LockSupport(diff(waiting, {t}), permits))
    

  LockSupportDet :: ({ThreadID}, {ThreadID}) -> Proc
  LockSupportDet(waiting, permits) = LockSupportDet1(waiting, permits)
  LockSupportDet1(waiting, permits) = ... -- Analogous to LockSupport1

exports

  channel spurious

  InitLockSupport = LockSupport({}, {})

  InitLockSupportDet = LockSupportDet({}, {})

  Park(t) = park.t -> wakeUp.t?_ ->  SKIP

  Unpark(t, t') = unpark.t.t' -> SKIP


endmodule _*$*_
\end{cspm}

% |InitLockSupportDet| is defined similarly to |InitLockSupport|; the only change is made by removing the nondeterministic choice for a thread to spuriously wakeup. This will allows us to show some divergence results later.



\paragraph{The ThreadInfo Queue}\label{section:ThreadInfo}

As seen in listing \ref{scala:Condition}, each |Condition| maintains a queue of |ThreadInfo| objects which have a thread id and a variable |ready| indicating whether the corresponding thread has been unparked. The natural method of modelling this queue in CSP is with a sequence of nodes, a number of processes each corresponding to a node \framebox{Wording etc} and some co-ordinator processes.

We first consider the |ThreadInfo| process. Each |ThreadInfo| process has a corresponding |Node| which remains unchanged throughout; this allows us to use the |Node| datatype to specify which |ThreadInfo| process a thread is interacting with. The |ThreadInfo(n)| process communicates over five channels:

\begin{itemize}
  \item |channel initialiseNode : Node . ThreadID|. A communication of |initaliseNode.n.t| is used to indicate that |ThreadInfo(n)| will act as thread |t|'s |threadInfo| object until it is released.
  \item |channel releaseNode : Node . ThreadID|. A communication on this will stop |ThreadInfo(n)| from acting as |t|'s |threadInfo| object, it can now be reinitialised by some thread.
  \item |channel setReady : Node . ThreadID . ThreadID| is used by some thread to indicate that thread |t| has been signalled; this is only allowed to occur once per initialisation.
  \item |channel isReady : Node . ThreadID . ThreadID . Bool| is used to indicate whether or not thread |t| has been signalled yet. It is used by |t| to protect against spurious wake-ups, reparking itself it is has awoken without being signalled.
  \item |channel nodeThread : Node . ThreadID . ThreadID|. A communication on this channel of the form |nodeThread.n.t.t2| indicates to thread |t2| that |ThreadInfo(n)| is currently initialised by thread |t|.
\end{itemize}

The lifecycle of the |ThreadInfo| process is that some thread |t| will initialise the |ThreadInfo| process with its identity after it is performing an |await|. The |ThreadInfo| process will then act as that thread's |threadInfo| object from the Scala implementaion. Its corresponding |Node| value will then be enqueued in the condition's queue. A thread may then dequeue that |Node| value vis either a |signal| or |signalAll| call; in both cases it will then call |setReady.n.t?_| to indicate that that the corresponding thread has been signalled. Once thread |t| has been unparked and awoken, it checks that it has been signalled via a communication on |isReady|. When this indicates that |t| has been signalled, |t| then communicates a |releaseNode| event to indicate that it has finished using the corresponding |ThreadInfo| process. It can then be re-initialised by some other thread and the above lifecycle repeated.
%Each of the nodes can be initialised by a thread, at which point it then acts as that thread's |ThreadInfo| object. It allows threads (including its allocated thread) to check the value of |ready| via a communication on the |isReady| channel, and for other threads to set the value of |ready| to true via the |setReady| channel. Once the parent thread has reawoken legitimately (i.e.~via an |Unpark| not a spurious wakeup) it then releases the node; this is valid as the node must have already been dequeued and |setReady|, hence no further communications should happen until it is reinitialised. We also introduce the channel |nodeThread : Node . ThreadID . ThreadID|; this indicates which thread has initialised the |ThreadInfo| node, corresponding to the Scala |threadInfo.thread|.
We always allow |ThreadInfo(n)| to communicate on any of its channels, however we diverge whenever some communication occurs that should not be possible in the original Scala code. This allows us to easily check that if the |ThreadInfo| processes are used correctly; any incorrect usage will lead to a divergence which we can find by checking for divergence-freedom later.

\begin{cspm}[caption={The definition of a {\scalastyle ThreadInfo} process}]
ThreadInfo :: (Node) -> Proc
ThreadInfo(n) = 
      initialiseNode.n?t -> ThreadInfoF(n, t)
  [] isReady.n?t?t2.true -> DIV
  [] setReady.n?t?t2 -> DIV
  [] nodeThread.n?t?t' -> DIV
  [] releaseNode.n?t -> DIV
ThreadInfoF(n, t) =  
      isReady.n!t?t2.false -> ThreadInfoF(n, t)
  [] setReady.n!t?t2:diff(ThreadID, t) -> ThreadInfoT(n, t)
  [] initialiseNode.n!t -> DIV
  [] nodeThread.n.t?t' -> ThreadInfoF(n, t)
  [] releaseNode.n?t -> DIV
ThreadInfoT(n, t) = 
      isReady.n!t?t2.true -> ThreadInfoT(n, t)
  [] setReady.n!t?t2 -> DIV
  [] initialiseNode.n!t -> DIV
  [] nodeThread.n.t?t' -> ThreadInfoT(n, t)
  [] releaseNode.n.t -> ThreadInfo(n)
\end{cspm}

Since we are handling |n| threads and all of them can be waiting at the same time, we hence need |n| |ThreadInfo| processes and hence we define the |Node| datatype as |datatype Node = N.{0..n-1}|. 
% Each of the nodes can be initialised by a thread, at which point it then acts as that thread's |ThreadInfo| object. It allows threads (including its allocated thread) to check the value of |ready| via a communication on the |isReady| channel, and for other threads to set the value of |ready| to true via the |setReady| channel. Once the parent thread has reawoken legitimately (i.e.~via an |Unpark| not a spurious wakeup) it then releases the node; this is valid as the node must have already been dequeued and |setReady|, hence no further communications should happen until it is reinitialised. We also introduce the channel |nodeThread : Node . ThreadID . ThreadID|; this indicates which thread has initialised the |ThreadInfo| node, corresponding to the Scala |threadInfo.thread|. The |ThreadInfo| objects are designed to diverge whenever a communcation occurs that should not be possible in the original Scala code.

% \begin{cspm}
%   ThreadInfo :: (Node) -> Proc
%   ThreadInfo(n) = 
%        initialiseNode.n?t -> ThreadInfoF(n, t)
%     [] isReady.n?t?t2.true -> DIV
%     [] setReady.n?t?t2 -> DIV
%     [] nodeThread.n?t?t' -> DIV
%     [] releaseNode.n?t -> DIV
%   ThreadInfoF(n, t) =  
%        isReady.n!t?t2.false -> ThreadInfoF(n, t)
%     [] setReady.n!t?t2:diff(ThreadID, t) -> ThreadInfoT(n, t)
%     [] initialiseNode.n!t -> DIV
%     [] nodeThread.n.t?t' -> ThreadInfoF(n, t)
%     [] releaseNode.n?t -> DIV
%   ThreadInfoT(n, t) = 
%        isReady.n!t?t2.true -> ThreadInfoT(n, t)
%     [] setReady.n!t?t2 -> DIV
%     [] initialiseNode.n!t -> DIV
%     [] nodeThread.n.t?t' -> ThreadInfoT(n, t)
%     [] releaseNode.n.t -> ThreadInfo(n)
% \end{cspm}


We then use a process called |NodeAllocator| to allocate the |Nodes| to threads and also to collect them when they are no longer required.
We note that any thread can use any node in this model; this is analogous to the nodes being memory chunks allocated to each thread by |NodeAllocator| and then garbage collected once they are no longer needed. 

We note that neither the |NodeAllocator| nor the |Node|s themselves take a |MonitorID| as a parameter; this is because each thread can only perform an |await| on one conition or monitor at any point in time. This choice allows us to use the same |Node|s across multiple monitors, leading to significantly smaller FDR state spaces when modelling using multiple monitors.

\begin{cspm}[caption={The {\scalastyle NodeAllocator} process}]
  NodeAllocator(ns) = 
    (not(empty(ns))) & 
      (initialiseNode$n:ns?t -> NodeAllocator(diff(ns, {n})))
  [] releaseNode?n:ns?t -> DIV
  [] releaseNode?n:diff(Node, ns)?t -> NodeAllocator(union(ns, {n}))_*$*_
\end{cspm}

Finally we have the |Queue| processes, which model the queues maintained inside each Condition. Each |Queue| keeps a sequence of the |Node| identities waiting on its condition. The |Queue| processes have three parameters: |m| is the identity of the SCL monitor and |c| is the identity of the condition that the queue belongs to; |qs| is the current state of the queue.%, with each Node corresponding to its current holding thread.

\begin{cspm}
  Queue(m, c, qs) = 
       (not(null(qs))) & dequeue.m.c?t!head(qs) -> Queue(m, c, tail(qs))
    [] (null(qs)) & isEmpty.m.c?t -> Queue(m, c, qs)
    [] enqueue.m.c?t?n:diff(Node, QS) -> Queue(m, c, qs^<n>) 
\end{cspm}

We have that each |Queue| is always ready to accept an |enqueue| (unless all nodes are already in the queue), but will only communicate one of |dequeue| or |isEmpty| at any point in time. We note that the restriction on the values of |n| that can be enqueued is such that the queue is of finite length; this is required for efficient model checking in FDR.

\subsubsection{The functions and interface of the monitor}

Now we have the components of the model of the monitor, the last step is to place these processes in parallel. The majority of these processes are independent of each other; only the |NodeAllocator| and the |ThreadInfo| processes need to synchronise with each other, which occurs when a node is either initialised or released.
\begin{cspm}
Queues(m, setC) = ||| c <- setC @ Queue(m, c, <>)
NodeSystem = 
  NodeAllocator(Node) 
    [|{|initialiseNode, releaseNode|}|] (||| n <- Node @ ThreadInfo(n))
    
InitialiseMon(m, setC) = 
  (Lock(m) ||| Queues(m, setC) ||| NodeSystem ||| InitLockSupport)
\end{cspm} 

The first function we export as part of the interface of our module is |runWith(P, mon, setC)| which each takes a process |P| representing program threads, an identity for the monitor and a set of conditions on that monitor. This processes synchronises |P| with the |InitialiseMon(m, setC)|. This is to allow the threads to `call' the various functions that act on the monitor correctly and so that mutual exclusion between threads can be obtained using the functions of the monitor. \framebox{Include spurious in next?}

\begin{cspm}
  ...
  -- The set of events that are hidden when a thread uses the monitor
  HideSet(m, setC) = 
    {|park, unpark, wakeUp, enqueue.m.c, dequeue.m.c, initialiseNode, nodeThread,
      setReady, isReady, isEmpty.m.c, releaseNode | c <- setC|}

  -- The set of events to synchronise on between a thread and the monitor
    SyncSet2(m, setC) = Union({{|acquire.m|}, {|release.m|}, HideSet(m, setC)})

exports

  channel spurious
  channel acquire, release: MonitorID.ThreadID
  channel callAcquire, callRelease: MonitorID.ThreadID
  channel callAwait, callSignalAll: MonitorID.ConditionID.ThreadID
  channel callSignal: MonitorID.ConditionID.ThreadID

  -- Runs the monitor with internal spurious wakeups
  runWith(P, mon, setC) = 
    ((P [|SyncSet(mon, setC)|] 
          InitialiseMon(mon, setC)) \ HideSet(mon, setC))

  -- Runs the monitor without internal spurious wakeups
  runWithDet(P, mon, setC) = 
    ((P [|SyncSet(mon, setC)|] 
          InitialiseMonDet(mon, setC)) \  HideSet(mon, setC))

  ...
\end{cspm}

In the definitions above, |SyncSet| contains every event that we need to synchronise on between a series of threads and the monitor. We then hide all events except for those representing a thread acquiring and releasing the lock; this is the contents of |HideSet|.

We now consider the functions offered by the monitor. We have the interface given below, with each of the five processes corresponding to the function of the same name. Each process starts with a communication on the correspondingly named |callX| channel to indicate that the specified thread has just called that function; this makes examining any traces produced significantly simpler. We will refer to these |callX| communications as `external' and all other channels as being `internal'. \framebox{Better convention for placeholder values?}
  
\begin{cspm}
export 
  ...

  -- Operations on the monitor
  Await(mon, cnd, t) = callAwait.mon.cnd.t -> Await1(mon, cnd, t)

  Signal(mon, cnd, t) = callSignal.mon.cnd.t -> Signal1(mon, cnd, t)

  SignalAll(mon, cnd, t) = callSignalAll.mon.cnd.t -> SignalAll1(mon, cnd, t)

  Lock(mon, t) = callAcquire.mon.t -> acquire.mon.t -> SKIP

  Unlock(mon, t) = callRelease.mon.t -> release.mon.t -> SKIP

 endmodule
\end{cspm}

Both |Lock| and |Unlock| both only require a single internal communication (either acquiring or releasing the lock) after the thread's external communication. By contrast |Await|, |Signal| and |SignalAll| are more complex, so the initial external communication is followed by another process, in each case named |X1|. Each of these processes are natural translations of the Scala code into CSP; the main exception is using |Await2| to represent the |while| loop in the Scala |await()| function.

\begin{cspm}
Signal1(mon, cnd, t) = 
      isEmpty.mon.cnd.t -> SKIP
  [] dequeue.mon.cnd.t?n -> nodeThread.n?t2!t -> isReady.n.t2.t?b ->
        (if b then Signal1(mon, cnd, t)
        else setReady.n.t2.t -> Unpark(t, t2))

SignalAll1(mon, cnd, t) =
      isEmpty.mon.cnd.t -> SKIP
  [] dequeue.mon.cnd.t?n -> nodeThread.n?t2!t -> setReady.n.t2.t -> 
          Unpark(t, t2); SignalAll1(mon, cnd, t)

Await1(mon, cnd, t) = 
  initialiseNode?n!t -> enqueue.mon.cnd.t.n -> release.mon.t -> Await2(mon, cnd, t, n)

Await2(mon, cnd, t, n) = 
  isReady.n.t.t?b -> (if b then releaseNode.n.t -> acquire.mon.t -> SKIP
                      else Park(t); Await2(mon, cnd, t, n))
\end{cspm}



\subsubsection{Correctness}\label{section:SCLMonitor-Correctness}

We now consider the correctness of our model. We present a specification process for a idealised monitor with conditions and perform refinement checks against it. We show that the ordering of |await|s is also upheld by a separate refinement check.
  
We will first consider the specification process of a monitor with multiple conditions. Each monitor process is parameterised over the identity of the monitor and a map of |ConditionID => {ThreadID}| representing the set of threads waiting on each |Condition|. We choose to use sets of waiting threads instead of queues of waiting threads to make this a more general specification of a monitor and it is also more efficient. We consider orderings in a later test. |initSet| is the initial mapping of the waiting threads, with each condition mapping to an empty set. We define |valuesSet| as a helper function which returns a set of all the threads that are currently waiting on any condition; this allows us to restrict the specification to only allow threads that aren't waiting to obtain the lock. 

\begin{cspm}
initSet = mapFromList(<(c, {}) | c <- seq(ConditionID)>)
valuesSet(map) = Union({mapLookup(map, cnd) | cnd <- ConditionID})
Spec2Unlocked(m, waiting) = 
  SCL::acquire.m?t:diff(ThreadID, values(waiting)) -> Spec2Locked(m, t, waiting)

Spec2Locked(m, t, waiting) =
  [] c': ConditionID @  
      (   
          (mapLookup(waiting, c') == {}) & SCL::callSignal.m.c'.t ->
             Spec2Locked(m, t, waiting)
       [] (mapLookup(waiting, c') != {}) & 
            |~| t': mapLookup(waiting, c') @ SCL::callSignal.m.c'.t -> 
                Spec2Locked(m, t, 
                            mapUpdate(waiting, c', diff(mapLookup(waiting, c'), {t'})))            
      )
  [] SCL::callSignalAll.m?c:ConditionID!t -> 
            Spec2Locked(m, t, mapUpdate(waiting, c, {}))
  [] SCL::callRelease.m.t -> SCL::release.m.t ->
        Spec2Unlocked(m, waiting)
  [] SCL::callAwait.m?c:ConditionID!t -> SCL::release.m.t ->
        Spec2Unlocked(m, mapUpdate(waiting, c, union(mapLookup(waiting, c), {t})))


Spec2Thread(t, m) = SCL::callAcquire.m.t -> SCL::acquire.m.t -> Spec2Thread2(t, m)
Spec2Thread2(t, m) = 
     SCL::callAwait.m?c.t -> SCL::release.m.t -> SCL::acquire.m.t -> Spec2Thread2(t, m)
  [] SCL::callRelease.m.t -> SCL::release.m.t -> Spec2Thread(t, m)
  [] SCL::callSignalAll.m?c.t -> Spec2Thread2(t, m)
  [] SCL::callSignal.m?c.t -> Spec2Thread2(t, m)

Spec2SCL = (let m = SigM.S.0 within 
  (Spec2Unlocked(m, initSet) [|SpecChans(m, ConditionID)|]
      (||| t <- ThreadID @ (Spec2Thread(t, m)))))
  \end{cspm}

% Here we have defined the processes where either the monitor lock is not held, or where it is held by thread |t| and is waiting to perform a function. We next define the processes where |t| is in the middle of waiting or releasing the lock.

%   \begin{cspm}
% -- t doing a wait; needs to release the lock
% SpecLockedWaiting(m, c, t, waiting, poss) =
%       SCL::release.m.t -> 
%         SpecUnlocked(m, mapUpdate(waiting, c, union(mapLookup(waiting, c), {t})), 
%                     poss)
%   [] SCL::callAcquire.m?t':diff(ThreadID, 
%                                 Union({values(waiting), poss, {t}})) ->
%         SpecLockedWaiting(m, c, t, waiting, union(poss, {t'}))

% -- t releasing the lock
% SpecLockedReleasing(m, t, waiting, poss) =
%       SCL::release.m.t -> SpecUnlocked(m, waiting, poss)
%   [] SCL::callAcquire.m?t':diff(ThreadID, 
%                                 Union({values(waiting), poss, {t}})) ->
%         SpecLockedReleasing(m, t, waiting, union(poss, {t'}))

% SpecSCL = (let m = SigM.S.0 within 
%             (SpecUnlocked(m, initSet, {}) 
%               [[callSignalSpec.m.c.t.t' <- SCL::callSignal.m.c.t 
%                   | c <- ConditionID, t <- ThreadID, t' <- ThreadID]]))
%   \end{cspm}

We first note that the specification process provided is divergence free; we choose this as an idealised monitor should never internally diverge. We use the technique of having multiple threads linear

To test against this specification, we interleave a number of process of |ThreadSCL(t)|, with each of these representing the potential (correct) usage of the monitor that thread |t| could perform. These are interleaved to form |ThreadsSCL| and then this is then synchronised with the SCL monitor, via the use of |runWith| or |runWithDet| as outlined above.

\begin{cspm}
ThreadSCL(t) = SCL::Lock(SigM.S.0, t); ThreadSCL1(t)
ThreadSCL1(t) =   
  [] c : ConditionID @ 
     (
          (SCL::Await(SigM.S.0, c, t); ThreadSCL1(t))      
      [] (SCL::Signal(SigM.S.0, c, t); ThreadSCL1(t))
      [] (SCL::Signal(SigM.S.0, c, t); ThreadSCL1(t))
      [] (SCL::SignalAll(SigM.S.0, c, t); ThreadSCL1(t))
     )
  [] (SCL::Unlock(SigM.S.0, t); ThreadSCL(t))

ThreadsSCL = ||| t<-ThreadID @ ThreadSCL(t)

SCLSystemSpur = SCL::runWith(ThreadsSCL, SigM.S.0, ConditionID)
SCLSystem = SCLSystemSpur \ {spurious}

assert SCLSystemSpur :[divergence free]
assert not SCLSystem :[divergence free]
\end{cspm}

We have that both the assertions pass: |SCLSystemSpur| is divergence free, but hiding the |spurious| channel introduces divergences. Since the system is divergence-free with the |spurious| channel visible, we can conclude that spurious wake-ups don't lead to a state where the system can diverge without more spurious wake-ups. We can therefore conclude that the divergences of the system with |spurious| hidden are only due to repeated spurious wakeups

% Similarly to \framebox{ref}, we have that this divergence is not a major concern since it relies on infrequent spurious wakeups occuring. We also note that, similarly to before in \framebox{ref to previous chapter}, we have that each of these states where a divergence can occur has a corresponding stable state, hence it is valid for us to check refinement under stable-failures in this case.

We note that using the stable failures-model is normally inappropriate for a system that can diverge. However, this is valid here as for any state that could be unstable due to repeated hidden spurious wakeups there exists a corresponding stable state where the non-deterministic choice in the |LockSupport| model blocks the |spurious| event. 

\begin{cspm}
  assert SpecSCL [F= (SCLSystem) 
\end{cspm}

We have that this the assertion holds, indicating that the SCL monitor fulfils the specification of a monitor as required.

We next consider the fairness of the monitor with regards to individual |signal| calls. In the SCL monitor, queues are used so that each |signal| wakes the thread that has been waiting for the longest time on the condition (if one exists). We test that this property holds using |OrderCheck|, a process which maintains a list of the threads waiting on each condition in the order that they started waiting. |OrderCheck| effectively acts as a watchdog, completing a communication over the new channel |error| if an error in ordering is detected; we therefore verify this has not occured by a trace refinement against the specification. 

We choose to use a watchdog as the left hand side of any assertion has to be normalised by FDR. Normalisation is an expensive process for complex specifications with many states; maintaining queues of waiting threads on each condition is therefore more suited to the right-hand side of an assertion as this does not require normalisation. For further details, see \cite{RoscoeUCS}.

\begin{cspm}
valuesSeq(map) = Union({set(mapLookup(map, cnd)) | cnd <- ConditionID})
channel error: MonitorID
OrderCheck(m, waiting) = 
      SCL::acquire.m?t:ThreadID -> 
      (if member(t, valuesSeq(waiting)) then error.m -> STOP--DIV
        else OrderCheck(m, waiting))
  [] SCL::callAwait.m?c?t -> 
      (if member(t, valuesSeq(waiting)) then error.m -> STOP
        else OrderCheck(m, mapUpdate(waiting, c, mapLookup(waiting, c)^<t>)))
  [] SCL::callSignalAll.m?c?_ -> 
        OrderCheck(m, mapUpdate(waiting, c, <>))
  [] SCL::callSignal.m?c?_ -> 
      (if null(mapLookup(waiting, c)) then OrderCheck(m, waiting)
        else OrderCheck(m, mapUpdate(waiting, c, 
                                    tail(mapLookup(waiting, c)))))
\end{cspm}

This new process only synchronises on the events ; this is sufficient to detect any threads which have non-spuriously woken up before they should.

To run the refinement checks, we place |OrderCheck| in parallel with |SCLSystem|. We only synchronise on events that that indicate a thread waking, waiting or acquiring the lock; these are all the communications offered by |OrderCheck| except for |error.m|. 

We then check that this still trace refines |SpecSCL|, which it does. We can therefore conclude that no |error| events occur and no new stable failures are introduced, hence the ordering within the model of the SCL monitor are maintained correctly.

\begin{cspm}
assert SpecSCL [T= (OrderCheck(SigM.S.0, initSeq) 
                     [|{|SCL::callAwait.SigM.S.0,
                         SCL::acquire.SigM.S.0,
                         SCL::callSignal.SigM.S.0,
                         SCL::callSignalAll.SigM.S.0|}|] SCLSystem)
\end{cspm}

\subsubsection{Limitations of natural model of the queue}

Though the model given above is a natural model of the SCL monitor, this is quite ill-suited to refinement checking in FDR. The current implementation of the |ThreadInfo| allows any thread to obtain and use any of the |Node|s; this leads to exponential blow up in the number of states as the number of threads increases. Considering a case where we have $n$ threads and $m$ are currently waiting with their nodes queued, this has $n\choose{m}$, or $O(n^{m})$ permutations.

We can instead use the same nodes, but restrict them so that each node |N.x| can only be used by the respective thread |T.x|. This forces each thread to use the same |ThreadInfo| each time, removing this source of blow up. This is most trivially done by changing |Await1| to specify the node to initialise and not a random one allocated by |NodeAllocator| i.e.~as follows:

\begin{cspm}
  Await1(mon, cnd, t) = initialiseNode.N.t -> ...

  NodeAllocator(ns) = 
      (not(empty(ns))) & (initialiseNode?n:ns?t -> NodeAllocator(diff(ns, {n}))) 
   [] ...
\end{cspm}

We will refer to this version of the queue as the `Simple' model. This simplified model has one possible bijective mapping of threads to nodes. By contrast, the natural model has $n!$ bijective mappings of threads to nodes. As a result, for every single state that the model with the simplified queue can be in, there are upto $n!$ states of the natural model that are identical in all manners other than the node allocations.

For further performance improvements, we can also remove the node allocator process as each node is pre-allocated. Additionally we can change the type signature of |Node| to |N.ThreadID| and simplify many of the channels (removing |nodeThread| and |releaseNode| entirely) as node indicates which thread it corresponds to as follows:

\begin{cspm}
  datatype Node = N.ThreadID
  channel enqueue: MonitorID.ConditionID.Node 
  channel dequeue: MonitorID.ConditionID.ThreadID.Node
  channel setReady: Node.ThreadID
  channel isReady: Node.ThreadID.Bool
  channel initialiseNode: Node
  channel isEmpty: MonitorID.ConditionID.ThreadID
  channel await, signalAll: MonitorID.ConditionID.ThreadID
\end{cspm}

All the definitions remain the same apart from removing any |nodeThread| and |releaseNode| communications and the required type changes\framebox{put raw code in an appendix?}. We keep |initialiseNode| to so that a thread can use it to indicate it is initialising a `new' |ThreadInfo| object and hence to reset the |ready| value to false. We also change |InitialiseMon| and |InitialiseMonDet| to remove the |NodeAllocator|; each of the individual |ThreadInfo| processes are still interleaved as before. We will refer to this as the `optimised' version.

We first need to check that this simplified model remains correct. To complete this, we repeat the same refinement checks as before. These still all pass, indicating that the monitor model with a modified queue fulfills the specification similarly\framebox{wording} to the natural queue model.

We next verify that the efficiency improvements occurs in practice too. We do this by running the FDR verification of |assert SpecSCL [F= SCLSystem| for a range of numbers of threads and conditions. We then compare the number of states generated by the natural queue model against the more efficient queues, with the results visible in table \ref{table:queue}.

\def\thickhline{\noalign{\hrule height 1.5pt}}

\begin{table}
  \renewcommand*{\arraystretch}{1.2}
  \caption{The number of states generated by FDR for the different queue implementations. The improvement value is given as the $\frac{\text{Original number of states}}{\text{Reduced number of states}}$}
    \begin{tabularx}{\linewidth}{|l|l|X|X|X|X|X|}
      \thickhline
      No.&No.& \multicolumn{5}{l|}{Number of states} \\
      threads&conditions& Natural & Simple & Improvement & optimised & Improvement\\
      \thickhline
      2 & 1 & 2288 & 1088 & 2.10& 904 & 2.53\\ \hline
      3 & 1 & 239428 & 36262 & 6.60& 26494 & 9.04 \\ \hline
      4 & 1 & 3.14$\times\text{10}^\text{7}$ & 1180416 & 26.7& 792240 & 39.7\\ \hline
      5 & 1 & 5.39$\times\text{10}^\text{9}$ & 4.06$\times\text{10}^\text{7}$ & 133& 2.59$\times\text{10}^\text{7}$ &208 \\
      \thickhline
      2 & 2 & 4932 & 2382 & 2.07& 2382 & 2.40\\ \hline
      3 & 2 & 686896 & 106672 & 6.44& 82973 & 8.27\\ \hline
      4 & 2 & 1.22$\times\text{10}^\text{8}$ & 4655652 & 26.2& 3363492 & 36.3\\ 
      \thickhline
      2 & 3 & 8436 & 4106 & 2.05& 3634 & 2.32\\ \hline
      3 & 3 & 1445008 & 227512 & 6.35 & 184276 & 7.84\\ \hline
      4 & 3 & 3.15$\times\text{10}^\text{8}$ & 1.22$\times\text{10}^\text{7}$ & 25.9& 9212868 & 34.2\\ 
      \thickhline
    \end{tabularx}
    %\vspace*{5mm}
    \label{table:queue}
  \end{table}

  Here we see that the restricted model with each thread allocated a single node to use results in a state space reduced by a factor of at least $n!$ where $n$ is the number of threads. This is as expected due to the reduction in the mappings from threads to nodes as stated above. Though the state space clearly still grows exponentially with the simplified queues, it is significantly more efficient and makes refinement checks for larger numbers of threads and conditions significantly more feasible.
  
  % If we consider the 
  
  
  % This is as expected: we have $n!$ possible allocations of threads to nodes (one per node, and with this weakly associating any unallocated node with some thread without an allocated node). Whenever a node |n| is allocated to thread |t| by an |initialiseNode|, we swap the thread that |n| was associated with to the node that |t| was previously weakly associated to. By contrast, the simplified queue has that each node can only ever be allocated/associated with a single node; there is only 1 permutation for this. As a result, for every single state that the model with the simplified queue can be in, there are $n!$ states of the natural model that are identical in all manners other than the node allocations.

  

  \framebox{Introduce efficient spec version of SCL monitor and compare performance?}.




  %If we consider the nodes in the natural implementation, we have that if $n'$ of the $n$ nodes have been allocated, then there are $(n-n')!$ possible allocations of the remaining nodes to the remaining threads. As a result, we can consider each of the $n$ nodes to be `paired' to one of the threads ny point in time, with this allocation 

   


  